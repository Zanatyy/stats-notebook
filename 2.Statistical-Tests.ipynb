{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elementary Statistical Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Familiarity with the Descriptive Statistics notebook.\n",
    "- Python Libraries such as:\n",
    "    - Numpy\n",
    "    - Scipy\n",
    "    - Sklearn\n",
    "    - Statsmodels\n",
    "    - Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elementary statistical testing refers to the basic methods used to make decisions or draw conclusions from data. These tests help determine whether observed patterns or differences are real or occurred by chance. Here are some key aspects:\n",
    "\n",
    "1. **Null and Alternative Hypothesis**: Like in all hypothesis testing, we start with the null hypothesis (H₀) that assumes no effect or no difference, and the alternative hypothesis (H₁) that suggests a real effect or difference.\n",
    "\n",
    "2. **Common Types of Tests**:\n",
    "   - **Z-test**: Used to compare sample data to the population when the sample size is large (typically \\(n > 30\\)) and the population variance is known.\n",
    "   - **T-test**: Used when the sample size is small or the population variance is unknown, typically to compare means.\n",
    "   - **Chi-square test**: Used for categorical data to test relationships or compare observed frequencies to expected ones.\n",
    "\n",
    "3. **Significance Level (α)**: Commonly set at 0.05, this is the threshold for determining if a result is statistically significant. If the result is below α, we reject the null hypothesis.\n",
    "\n",
    "4. **P-value**: The p-value tells us how likely it is that the observed data would occur if the null hypothesis were true. A small p-value (less than α) suggests that the observed result is unlikely to be due to chance alone, leading us to reject the null hypothesis.\n",
    "\n",
    "Elementary statistical testing is foundational in analyzing data, used in everything from comparing averages to examining relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n",
    "Hypothesis testing is a way to make decisions or draw conclusions about a population based on data from a sample. It helps us figure out if something we observe (like a difference or an effect) is real or just happened by chance.\n",
    "\n",
    "1. **Null Hypothesis (H₀)**: This is a starting assumption that nothing has changed, or there’s no effect. We assume it's true until we have evidence to prove otherwise.\n",
    "\n",
    "2. **Alternative Hypothesis (H₁)**: This is what we're trying to prove — that there is a change, effect, or difference.\n",
    "\n",
    "We collect data, calculate a test result, and then decide whether the evidence is strong enough to reject the null hypothesis and support the alternative hypothesis.\n",
    "\n",
    "3. **Test Statistic**: A test statistic is calculated from the sample data and is used to assess the strength of the evidence against the null hypothesis. The type of test statistic depends on the hypothesis test being conducted. (ex: t-statistic)\n",
    "\n",
    "4. **Significance Level**: The threshold for the t-statistic to reject the null hypothesis. Commonly used thresholds are 5% or 1%.\n",
    "\n",
    "5. **P-Value**:  The p-value is the probability of obtaining a test statistic as extreme as the one observed, assuming the null hypothesis is true. If the p-value is less than the significance level α, we reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Tests:\n",
    "Most commonly used tests are testing for a mean. The goal is to make inferences about the population mean based on sample data. Commonly used tests for a mean are:\n",
    "\n",
    "#### **1. T-Test/Z-Test**: The t-test and z-test are both statistical tests used to determine if there are significant differences between means.\n",
    "\n",
    "Before using we have to assume that the observations are independent and come from a Normal distribution.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{X} - \\mu_0}{s / \\sqrt{n}}\n",
    "$$\n",
    "\n",
    "We can also retrieve the t-statistic from the table by calculating Degrees of Freedom (df):\n",
    "\n",
    "$$ {df} = {n}-1 $$\n",
    "as ${n}$ is number of samples\n",
    "\n",
    "The critical value of t-statistic should look like this:\n",
    "\n",
    "Degrees of Freedom | 0.10  | 0.05  | 0.01  | 0.001 |\n",
    "|------------------|-------|-------|-------|-------|\n",
    "1                  | 6.314 | 12.71 | 63.66 | 636.62|\n",
    "2                  | 2.920 | 4.303 | 9.925 | 22.327|\n",
    "3                  | 2.353 | 3.182 | 5.841 | 10.215|\n",
    "4                  | 2.132 | 2.776 | 4.604 | 7.173 |\n",
    "5                  | 2.015 | 2.571 | 4.032 | 5.893 |\n",
    "6                  | 1.943 | 2.447 | 3.707 | 5.208 |\n",
    "7                  | 1.895 | 2.364 | 3.499 | 4.785 |\n",
    "8                  | 1.860 | 2.306 | 3.355 | 4.501 |\n",
    "9                  | 1.833 | 2.262 | 3.250 | 4.297 |\n",
    "10                 | 1.812 | 2.228 | 3.169 | 4.144 |\n",
    "\n",
    "We use T-Test when the number of samples < 30 or when the standard deviation is unknown, Otherwise Z-Test is used.\n",
    "\n",
    "**Central Limit Theorem (CLT)**: The theorem states that the distribution of the sample mean will approach a normal distribution as the sample size becomes large, regardless of the original population's distribution. According to the theorem we use T-Test when we dont know the standard deviation.\n",
    "\n",
    "**Types of the test depending on the sample**:\n",
    "\n",
    "##### 1. **One-Sample t-Test**:\n",
    "   - **Objective**: Test if the sample mean is significantly different from a known value (e.g., a population mean).\n",
    "   - **When to Use**: When you have one sample and want to compare its mean to a known value.\n",
    "   - **Code**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -1.2923246855119326\n",
      "p-value: 0.2658472860753787\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "data = [2.3, 2.9, 3.1, 2.8, 3.0]\n",
    "\n",
    "# Hypothesized population mean\n",
    "population_mean = 3.0\n",
    "\n",
    "# Perform one-sample t-test\n",
    "t_stat, p_value = stats.ttest_1samp(data, population_mean)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 2. **Independent Samples t-Test**:\n",
    "   - **Objective**: Compare the means of two independent groups to determine if they are significantly different.\n",
    "   - **When to Use**: When you have two separate groups and want to compare their means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -3.8247315498700623\n",
      "p-value: 0.00505561083180125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data for two groups\n",
    "group1 = [2.3, 2.9, 3.1, 2.8, 3.0]\n",
    "group2 = [3.2, 3.5, 3.6, 3.3, 3.7]\n",
    "\n",
    "# Perform independent two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 3. **Paired Samples t-Test (Dependent)**:\n",
    "   - **Objective**: Compare means from the same group at different times or under different conditions.\n",
    "   - **When to Use**: When you have paired or matched observations, such as before-and-after measurements on the same subjects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -8.500000000000004\n",
      "p-value: 0.0010505780707892483\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data before and after treatment\n",
    "before = [2.3, 2.9, 3.1, 2.8, 3.0]\n",
    "after = [2.8, 3.2, 3.4, 3.1, 3.3]\n",
    "\n",
    "# Perform paired sample t-test\n",
    "t_stat, p_value = stats.ttest_rel(before, after)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **2. ANOVA Test**: An extension for T-Test for more than two groups\n",
    "ANOVA (Analysis of Variance) is a statistical test used to determine if there are significant differences among the means of three or more groups. It helps assess whether at least one group mean is different from the others, based on sample data.\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "- **Purpose**: To compare the means of multiple groups to see if there are statistically significant differences between them.\n",
    "- **Types**:\n",
    "  - **One-Way ANOVA**: Tests differences among the means of three or more independent groups based on one factor (e.g., comparing the effectiveness of three different teaching methods).\n",
    "  - **Two-Way ANOVA**: Tests differences among means based on two factors and their interaction (e.g., examining the effects of teaching method and class size on student performance).\n",
    "\n",
    "**Assumptions**\n",
    "1. **Independence**: Observations in each group should be independent of each other.\n",
    "2. **Normality**: Data in each group should be approximately normally distributed (especially important for small sample sizes).\n",
    "3. **Homogeneity of Variances**: The variances among the groups should be roughly equal.\n",
    "\n",
    "**Procedure**\n",
    "1. **Formulate Hypotheses**:\n",
    "   - **Null Hypothesis (H0)**: All group means are equal.\n",
    "   - **Alternative Hypothesis (H1)**: At least one group mean is different.\n",
    "\n",
    "2. **Calculate F-Statistic**: Compares the variance between groups to the variance within groups.\n",
    "\n",
    "3. **Compare F-Statistic to Critical Value**: Determine if the observed F-Statistic is significantly large compared to a critical value from the F-distribution.\n",
    "\n",
    "ANOVA is widely used in experimental designs and research to evaluate the effects of different treatments or conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Non-parametric Tests**\n",
    "Non-parametric tests are statistical methods used when data doesn't meet the assumptions of parametric tests, such as normal distribution or equal variances. They rely on the ranks of the data rather than the raw data values. These tests are ideal for ordinal, skewed, or non-normally distributed data\n",
    "\n",
    "Common tests used are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Chi-Square**\n",
    "The Chi-Square Test is a non-parametric test used to assess the association between categorical variables or to compare observed frequencies with expected frequencies. There are two primary types of Chi-Square Tests:\n",
    "\n",
    "#### **1. Chi-Square Test of Independence**\n",
    "\n",
    "**Purpose**: To determine if there is a significant association between two categorical variables.\n",
    "\n",
    "**Example**: Testing if there is a relationship between gender (male/female) and voting preference (yes/no).\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **Create a Contingency Table**: Construct a table that displays the frequency counts for each combination of categories for the two variables.\n",
    "\n",
    "2. **Calculate Expected Frequencies**: For each cell in the table, calculate the expected frequency under the assumption that the variables are independent. The formula is:\n",
    "   $$\n",
    "   E_{ij} = \\frac{(R_i \\cdot C_j)}{N}\n",
    "   $$\n",
    "   Where:\n",
    "   - $E_{ij}$ = Expected frequency for cell $(i, j)$\n",
    "   - $R_{i}$ = Total frequency for row $(i)$\n",
    "   - $C_{J}$ = Total frequency for column $(j)$\n",
    "   - ${N}$ = Total number of observations\n",
    "\n",
    "3. **Compute the Chi-Square Statistic**:\n",
    "The Chi-Square statistic is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $O_{ij}$ = Observed frequency for cell $(i, j)$\n",
    "- $E_{ij}$ = Expected frequency for cell $(i, j)$\n",
    "\n",
    "\n",
    "4. **Determine Degrees of Freedom**: Calculate the degrees of freedom using:\n",
    "   $$\n",
    "   \\text{df} = (r - 1) \\times (c - 1)\n",
    "   $$\n",
    "   Where:\n",
    "   - r = Number of rows\n",
    "   - c = Number of columns\n",
    "\n",
    "5. **Compare to Critical Value**: Compare the calculated \\(\\chi^2\\) value to the critical value from the Chi-Square distribution table with the appropriate degrees of freedom or compute the p-value.\n",
    "\n",
    "6. **Draw Conclusions**: If the p-value is less than the significance level (e.g., 0.05), reject the null hypothesis and conclude that there is a significant association between the variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Statistic: 15.041666666666666\n",
      "p-Value: 0.00010516355403363098\n",
      "Degrees of Freedom: 1\n",
      "Expected Frequencies Table:\n",
      "[[20. 20.]\n",
      " [30. 30.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Example contingency table: rows are gender (Male, Female) and columns are voting preferences (Yes, No)\n",
    "data = np.array([[30, 10],   # Male\n",
    "                 [20, 40]])  # Female\n",
    "\n",
    "# Perform Chi-Square Test of Independence\n",
    "chi2_stat, p_value, dof, expected = stats.chi2_contingency(data)\n",
    "\n",
    "print(\"Chi-Square Statistic:\", chi2_stat)\n",
    "print(\"p-Value:\", p_value)\n",
    "print(\"Degrees of Freedom:\", dof)\n",
    "print(\"Expected Frequencies Table:\")\n",
    "print(expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **2. Chi-Square Test of Goodness of Fit**\n",
    "\n",
    "**Purpose**: To determine if a sample distribution matches an expected distribution.\n",
    "\n",
    "**Example**: Testing if the observed distribution of colors in a bag of candies fits the expected proportions.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **State the Expected Frequencies**: Define the expected proportions or frequencies based on a theoretical distribution.\n",
    "\n",
    "2. **Compute the Chi-Square Statistic**:\n",
    "   $$\n",
    "   \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n",
    "   $$\n",
    "   Where:\n",
    "   - $O_i$ = Observed frequency for category $i$\n",
    "   - $E_i$ = Expected frequency for category $i$\n",
    "3. **Determine Degrees of Freedom**: Calculate the degrees of freedom using:\n",
    "   $$\n",
    "   \\text{df} = k - 1\n",
    "   $$\n",
    "   Where:\n",
    "   - $k$ = Number of categories\n",
    "\n",
    "4. **Compare to Critical Value**: Compare the calculated $\\chi^2$ value to the critical value from the Chi-Square distribution table with the appropriate degrees of freedom or compute the p-value.\n",
    "\n",
    "5. **Draw Conclusions**: If the p-value is less than the significance level, reject the null hypothesis and conclude that the observed distribution differs significantly from the expected distribution.\n",
    "\n",
    "The Chi-Square Test is widely used in categorical data analysis and helps determine if there are significant deviations from what is expected or if there is an association between categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Statistic: 15.714285714285715\n",
      "p-Value: 0.0012976436580202741\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Observed frequencies of candies (e.g., Red, Blue, Green, Yellow)\n",
    "observed = np.array([20, 30, 10, 10])\n",
    "\n",
    "# Expected frequencies (assuming equal distribution)\n",
    "expected = np.array([17.5, 17.5, 17.5, 17.5])\n",
    "\n",
    "# Perform Chi-Square Test of Goodness of Fit\n",
    "chi2_stat, p_value = stats.chisquare(f_obs=observed, f_exp=expected)\n",
    "\n",
    "print(\"Chi-Square Statistic:\", chi2_stat)\n",
    "print(\"p-Value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next tests use rank sums instead of the difference in the mean of the sample. The advantage of taking the rank sums rather than the difference in means is that the data need not be normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2. Mann-Whitney U Test**\n",
    "\n",
    "- **Purpose**: To compare differences between two independent groups when the data is not normally distributed.\n",
    "- **Example**: Comparing scores from two different teaching methods.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  U = R - \\frac{n(n + 1)}{2}\n",
    "  $$\n",
    "  Where \\(R\\) is the sum of ranks for the group and \\(n\\) is the number of observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney U Statistic: 9.5\n",
      "p-Value: 0.6004018480969686\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Sample data for two independent groups\n",
    "group1 = [23, 45, 67, 89, 12]\n",
    "group2 = [34, 56, 78, 90, 23]\n",
    "\n",
    "# Perform the Mann-Whitney U test\n",
    "u_statistic, p_value = stats.mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "\n",
    "# Print results\n",
    "print(f\"Mann-Whitney U Statistic: {u_statistic}\")\n",
    "print(f\"p-Value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3. Wilcoxon Signed-Rank Test**\n",
    "\n",
    "- **Purpose**: To compare two related samples or paired observations.\n",
    "- **Example**: Comparing pre-treatment and post-treatment scores from the same subjects.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  W = \\text{min}(W^+, W^-)\n",
    "  $$\n",
    "  Where \\(W^+\\) and \\(W^-\\) are the sums of ranks for positive and negative differences, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon Signed-Rank Statistic: 3.0\n",
      "p-Value: 0.4614509878333607\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Sample data: paired observations before and after treatment\n",
    "before = [15, 20, 18, 24, 29]\n",
    "after = [21, 25, 18, 23, 28]\n",
    "\n",
    "# Perform the Wilcoxon Signed-Rank Test\n",
    "statistic, p_value = stats.wilcoxon(before, after)\n",
    "\n",
    "# Print results\n",
    "print(f\"Wilcoxon Signed-Rank Statistic: {statistic}\")\n",
    "print(f\"p-Value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **4. Kruskal-Wallis H Test**\n",
    "\n",
    "- **Purpose**: To compare medians among three or more independent groups. Ideal usage is when the data is not normally distributed.\n",
    "- **Example**: Comparing median salaries among different job positions.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  H = \\frac{12}{N(N + 1)} \\sum \\frac{R_j^2}{n_j} - 3(N + 1)\n",
    "  $$\n",
    "  Where $(R_j)$ is the sum of ranks for group $(j)$, $(n_j)$ is the number of observations in group $(j)$, and $(N)$ is the total number of observations.\n",
    "  \n",
    "  The test is very similar to ANOVA tests but instead of using the mean of every dataset we sort the data by ranks then we use it to calculate our statistic so we compare them to each other. Which gives us an advantage of being able to use it when ANOVA assumptions are violated or the data is not normally distributed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kruskal-Wallis H Statistic: 0.5\n",
      "p-value: 0.7788007830714049\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data for three independent groups\n",
    "group1 = [10, 20, 30, 40, 50]\n",
    "group2 = [15, 25, 35, 45, 55]\n",
    "group3 = [12, 22, 32, 42, 52]\n",
    "\n",
    "# Perform Kruskal-Wallis H Test\n",
    "statistic, p_value = stats.kruskal(group1, group2, group3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Kruskal-Wallis H Statistic: {statistic}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **5. Friedman Test**\n",
    "\n",
    "- **Purpose**: To compare medians among three or more related groups.\n",
    "- **Example**: Comparing ratings of different products by the same group of people over time.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\chi^2_F = \\frac{12}{n k (k + 1)} \\left( \\sum_{j=1}^k R_j^2 - \\frac{k (k + 1)^2}{4} \\right)\n",
    "  $$\n",
    "  Where \\(R_j\\) is the sum of ranks for treatment \\(j\\), \\(n\\) is the number of subjects, and \\(k\\) is the number of treatments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman Statistic: 6.0\n",
      "p-value: 0.04978706836786395\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data: three sets of paired data (e.g., measurements at different times or treatments)\n",
    "group1 = [10, 20, 30]\n",
    "group2 = [15, 25, 35]\n",
    "group3 = [12, 22, 32]\n",
    "\n",
    "# Perform Friedman Test\n",
    "statistic, p_value = stats.friedmanchisquare(group1, group2, group3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Friedman Statistic: {statistic}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **6. Spearman’s Rank Correlation**\n",
    "\n",
    "- **Purpose**: To measure the strength and direction of the monotonic relationship between two variables.\n",
    "- **Example**: Assessing the relationship between the ranks of students' performances in two subjects.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\rho = 1 - \\frac{6 \\sum d_i^2}{n (n^2 - 1)}\n",
    "  $$\n",
    "  Where \\(d_i\\) is the difference between the ranks of each pair of observations, and \\(n\\) is the number of observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Correlation Coefficient: 0.9999999999999999\n",
      "p-value: 1.4042654220543672e-24\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data for two variables\n",
    "x = [10, 20, 30, 40, 50]\n",
    "y = [12, 24, 33, 47, 55]\n",
    "\n",
    "# Calculate Spearman's correlation\n",
    "corr, p_value = stats.spearmanr(x, y)\n",
    "\n",
    "# Print results\n",
    "print(f\"Spearman's Correlation Coefficient: {corr}\")\n",
    "print(f\"p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Kendall’s Tau-a**\n",
    "\n",
    "- **Purpose**: To measure the strength and direction of association between two ranked variables, providing a non-parametric alternative to Pearson's and Spearman's correlation.\n",
    "- **Example**: Assessing the relationship between rankings of two different judges on a set of performances.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\tau_a = \\frac{C - D}{\\frac{1}{2} n(n - 1)} = \\frac{C - D}{C + D}\n",
    "  $$\n",
    "  Where \\(C\\) is the number of concordant pairs, \\(D\\) is the number of discordant pairs, and \\(n\\) is the number of observations.\n",
    "  \n",
    "Kendall's Tau-a is particularly useful for ordinal data or non-normally distributed continuous data, making it robust against outliers and ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendall's Tau-a: 0.7378647873726218, p-value: 0.07697417298126674\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([5, 6, 7, 8, 7])\n",
    "\n",
    "# Calculate Kendall's Tau\n",
    "tau, p_value = kendalltau(x, y)\n",
    "\n",
    "print(f\"Kendall's Tau-a: {tau}, p-value: {p_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
